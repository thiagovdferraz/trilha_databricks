Perfeito ‚Äî com base em todas as imagens enviadas, aqui est√° a vers√£o atualizada do **README da Aula 05 ‚Äì Lakeflow Jobs** para a **Trilha Databricks**, j√° traduzido, estruturado e adaptado ao formato das aulas anteriores:

---

# üß≠ Aula 05 ‚Äî Lakeflow Jobs

## üéØ Objetivo da Aula

Nesta aula, vamos entender como o **Lakeflow Jobs** atua como o componente de **orquestra√ß√£o unificada** dentro da plataforma Databricks, permitindo automatizar e coordenar **qualquer tipo de workload** ‚Äî incluindo *notebooks*, *consultas SQL*, *dashboards*, *pipelines* e *workflows de machine learning* ‚Äî tudo dentro do mesmo ambiente do Lakehouse.

---

## üß© Por que aprender Lakeflow Jobs?

Muitas empresas usam orquestradores externos como **Airflow**, **Prefect** ou **Dagster**. Apesar de populares, eles criam desafios s√©rios:

* ‚ùå S√£o dif√≠ceis de usar e manter;
* ‚ùå Aumentam o custo operacional e reduzem a confiabilidade;
* ‚ùå N√£o s√£o integrados nativamente ao **Lakehouse**, gerando silos e problemas de integra√ß√£o.

O **Lakeflow Jobs** resolve esses problemas ao oferecer **uma orquestra√ß√£o totalmente integrada** ao Databricks ‚Äî conectando ingest√£o, transforma√ß√£o, governan√ßa e machine learning em um s√≥ fluxo.

---

## üß± Arquitetura do Lakeflow Jobs

O Lakeflow Jobs √© composto por quatro blocos principais:

1. **Triggers (gatilhos)**

   * Executa jobs de forma **agendada**, **cont√≠nua**, por **chegada de arquivo** ou **atualiza√ß√£o de tabela**.

2. **Control Flow (fluxo de controle)**

   * Gerencia **depend√™ncias e condi√ß√µes de execu√ß√£o** entre tarefas dentro do workflow.

3. **Observability (observabilidade)**

   * Monitora execu√ß√µes, falhas e m√©tricas para depura√ß√£o e confiabilidade.

4. **Compute (processamento)**

   * Executa workloads em clusters otimizados de acordo com o tipo de tarefa: ETL, ML/AI, ou Analytics/BI.

---

## ‚öôÔ∏è Lakeflow Jobs dentro da Plataforma Databricks

O **Lakeflow** unifica toda a engenharia de dados dentro da **Data Intelligence Platform**, conectando:

| Camada                                     | Fun√ß√£o                                     |
| ------------------------------------------ | ------------------------------------------ |
| **Connect**                                | Conectores eficientes de ingest√£o          |
| **Declarative Pipelines (DLT)**            | Desenvolvimento acelerado de ETL           |
| **Jobs (Workflows)**                       | Orquestra√ß√£o confi√°vel para analytics e IA |
| **Processing Engine (Photon)**             | Execu√ß√£o de alto desempenho                |
| **Governan√ßa (Unity Catalog)**             | Controle de seguran√ßa e lineage            |
| **Storage (Delta Lake, Parquet, Iceberg)** | Camada de armazenamento otimizada          |

---

## üöÄ Benef√≠cios principais

* **Autorias simples** ‚Üí Crie e gerencie workflows em minutos
* **Insights acion√°veis** ‚Üí Monitore execu√ß√µes e depend√™ncias facilmente
* **Confiabilidade comprovada** ‚Üí Execu√ß√£o nativa, segura e resiliente

---

## üß† O que voc√™ vai aprender

* Entender o papel do Lakeflow Jobs na arquitetura Databricks.
* Projetar workloads usando **DAGs (Directed Acyclic Graphs)**.
* Configurar **gatilhos de execu√ß√£o** (manual, agendado, file arrival, cont√≠nuo).
* Implementar **depend√™ncias condicionais e execu√ß√µes autom√°ticas**.
* Aplicar **boas pr√°ticas de orquestra√ß√£o** e tratamento de erros em produ√ß√£o.

---

## üß™ Pr√°tica da Aula

* Criar um **workflow completo** com m√∫ltiplas tarefas (notebook + SQL).
* Configurar **depend√™ncias e triggers** entre elas.
* Monitorar execu√ß√£o e logs diretamente no Lakeflow.
* Simular falhas e executar **repair runs** para garantir toler√¢ncia a erros.

---

## üì¶ Resultado Esperado

Ao final da aula, voc√™ ser√° capaz de:

* Criar e agendar pipelines no Databricks usando Lakeflow Jobs;
* Integrar diferentes tipos de workloads (SQL, ETL, ML, dashboards);
* Aplicar fluxos de controle e observabilidade em pipelines reais;
* Entender como o Lakeflow substitui orquestradores externos, simplificando a arquitetura de dados.

---

## üìã Exemplo Pr√°tico 1: Ingest√£o Batch com SQL Scripts

### Objetivo
Criar um workflow de ingest√£o batch que processa 3 arquivos CSV (claims, customers, policies) do volume `00_landing` para a camada `01_bronze` usando scripts SQL simples.

### Pr√©-requisitos

1. ‚úÖ Cat√°logo e schemas criados (notebook `01_create_catalog_and_schemas.ipynb`)
2. ‚úÖ Volumes criados e dados carregados (notebook `02_create_volumes_and_load_data.ipynb`)
3. ‚úÖ Arquivos CSV dispon√≠veis em `/Volumes/smart_claims_dev/00_landing/sql_server/`

### Arquivos de Ingest√£o

Os scripts de ingest√£o est√£o localizados em `source_to_bronze/`:

* **`ingestion_claim.py`** - Ingesta dados de `claims*.csv` para `smart_claims_dev.01_bronze.claims` (PySpark)
* **`ingestion_claim.sql`** - Vers√£o SQL alternativa (mantida para refer√™ncia)
* **`ingestion_customers.sql`** - Ingesta dados de `customers.csv` para `smart_claims_dev.01_bronze.customers`
* **`ingestion_policies.sql`** - Ingesta dados de `policies.csv` para `smart_claims_dev.01_bronze.policies`

### Estrutura dos Scripts

Os scripts SQL seguem o padr√£o:
1. Selecionam o cat√°logo e schema corretos
2. Removem a tabela existente (se houver) para permitir re-ingest√£o
3. Criam a tabela bronze usando `CREATE TABLE AS` com `read_files()`

O script Python (`ingestion_claim.py`) segue padr√£o similar usando PySpark:
1. L√™ vari√°veis via widgets ou vari√°veis de ambiente
2. Seleciona cat√°logo e schema
3. Remove a tabela existente (se houver)
4. L√™ arquivos CSV usando `spark.read` com padr√£o glob
5. Salva como tabela Delta

Exemplo Python (`ingestion_claim.py`):

```python
catalog = _get_widget_or_env("catalog", "CATALOG")
schema = _get_widget_or_env("schema", "SCHEMA")

spark.sql(f"USE CATALOG {catalog}")
spark.sql(f"USE SCHEMA {schema}")

volume_path = f"/Volumes/{catalog}/00_landing/sql_server/claims*.csv"
claims_df = spark.read.format("csv").option("header", "true").load(volume_path)
claims_df.write.mode("overwrite").saveAsTable(f"{catalog}.{schema}.claims")
```

Exemplo SQL (`ingestion_claim.sql`):

```sql
USE CATALOG smart_claims_dev;
USE SCHEMA 01_bronze;

DROP TABLE IF EXISTS smart_claims_dev.01_bronze.claims;

CREATE TABLE smart_claims_dev.01_bronze.claims
AS SELECT *
FROM read_files(
  '/Volumes/smart_claims_dev/00_landing/sql_server/claims*.csv',
  format => 'csv'
);
```

### Pr√≥ximos Passos

1. **Executar os scripts individualmente** para validar a ingest√£o
2. **Criar um Lakeflow Job** com 3 tasks:
   * 1 task Python para `ingestion_claim.py`
   * 2 tasks SQL para `ingestion_customers.sql` e `ingestion_policies.sql`
3. **Configurar depend√™ncias** se necess√°rio (embora neste caso as 3 tasks podem executar em paralelo)
4. **Configurar triggers** (agendado, manual ou file arrival)

### Reprocessamento de M√∫ltiplos Arquivos com Vari√°veis

Os scripts `ingestion_claim.py` e `ingestion_claim.sql` foram configurados para ser **gen√©ricos e processar m√∫ltiplos arquivos automaticamente**:

* **Vari√°veis de Task:** Usa `${catalog}` e `${schema}` passadas como vari√°veis do Lakeflow Job
* **Arquivos processados:** `claims*.csv` (todos automaticamente via padr√£o glob)
* **M√©todo:** Usa padr√£o glob `claims*.csv` no `read_files()` para processar todos os arquivos automaticamente
* **Reprocessamento:** O script sempre reprocessa TODOS os arquivos quando executado (DROP + CREATE)

**Configura√ß√£o no Lakeflow Job:**

Na task que executa o script (Python ou SQL), configure as seguintes vari√°veis:

| Vari√°vel   | Valor Exemplo    | Descri√ß√£o                    |
|------------|------------------|------------------------------|
| `catalog`  | `smart_claims_dev`| Nome do cat√°logo Unity Catalog|
| `schema`    | `01_bronze`      | Nome do schema de destino    |

**Exemplo de uso:**

1. Adicione novos arquivos CSV no volume (ex: `claims_03.csv`, `claims_04.csv`)
2. Execute o script novamente - ele processar√° **automaticamente** todos os arquivos `claims*.csv`
3. N√£o √© necess√°rio modificar o script quando novos arquivos chegam

**Estrutura do script gen√©rico (Python):**

```python
catalog = _get_widget_or_env("catalog", "CATALOG")
schema = _get_widget_or_env("schema", "SCHEMA")

spark.sql(f"USE CATALOG {catalog}")
spark.sql(f"USE SCHEMA {schema}")

volume_path = f"/Volumes/{catalog}/00_landing/sql_server/claims*.csv"
target_table = f"{catalog}.{schema}.claims"

spark.sql(f"DROP TABLE IF EXISTS {target_table}")
claims_df = spark.read.format("csv").option("header", "true").load(volume_path)
claims_df.write.mode("overwrite").saveAsTable(target_table)
```

**Estrutura do script gen√©rico (SQL):**

```sql
USE CATALOG ${catalog};
USE SCHEMA ${schema};

DROP TABLE IF EXISTS ${catalog}.${schema}.claims;

CREATE TABLE ${catalog}.${schema}.claims
AS SELECT *
FROM read_files(
  '/Volumes/${catalog}/00_landing/sql_server/claims*.csv',
  format => 'csv'
);
```

**Vantagens desta abordagem:**

* ‚úÖ **Gen√©rico:** Funciona para qualquer cat√°logo/schema via vari√°veis
* ‚úÖ **Autom√°tico:** Processa todos os arquivos `claims*.csv` sem modifica√ß√£o manual
* ‚úÖ **Escal√°vel:** Novos arquivos s√£o processados automaticamente
* ‚úÖ **Reutiliz√°vel:** Mesmo script pode ser usado em diferentes ambientes

---

## üìã Exemplo Pr√°tico 2: Transforma√ß√£o Bronze ‚Üí Silver

### Objetivo
Criar um workflow de transforma√ß√£o que processa dados da camada bronze para a camada silver, incluindo deduplica√ß√£o e enriquecimento de dados.

### Pr√©-requisitos
1. ‚úÖ Dados j√° ingeridos na camada bronze (claims, customers, policies)
2. ‚úÖ Schemas bronze e silver criados no cat√°logo

### Estrutura do Workflow

O processo bronze_to_silver agora consiste em **4 tasks sequenciais**:

#### Task 1: Deduplica√ß√£o de Claims (`01_deduplicate_claims.py`)

**O que faz:**

* Remove registros duplicados da tabela `claims` baseado na chave `claim_no`
* Mant√©m apenas uma c√≥pia de cada registro (a mais recente por `claim_date`)
* Calcula m√©tricas antes e depois da deduplica√ß√£o dentro da pr√≥pria task Python
* Cria a tabela `claims_dedup` na camada silver

**Vari√°veis de Task necess√°rias:**

* `${catalog}` - Nome do cat√°logo
* `${schema_bronze}` - Schema bronze (ex: `01_bronze`)
* `${schema_silver}` - Schema silver (ex: `02_silver`)

**Sa√≠da:**

* Tabela: `${catalog}.${schema_silver}.claims_dedup`
* M√©tricas mostradas no output da task e registradas via `taskValues`

#### Tasks 2 a 4: Join das 3 Tabelas (`02_join_tables_create.sql`, `03_join_tables_metrics.sql`, `04_join_tables_sample.sql`)

**O que fazem:**

* Task 2 cria a tabela `claims_enriched` com o join das 3 tabelas:
  * `claims_dedup` (deduplicados da task 1)
  * `customers` (dados de clientes)
  * `policies` (dados de ap√≥lices)
* Task 3 calcula m√©tricas de volume e unicidade da tabela enriquecida
* Task 4 retorna uma amostra dos dados para valida√ß√£o visual

**L√≥gica do JOIN (Task 2):**

1. `claims_dedup` JOIN `policies` ON `claims_dedup.policy_no = policies.POLICY_NO`
2. `policies` JOIN `customers` ON `policies.CUST_ID = customers.customer_id`

**Vari√°veis de Task necess√°rias:**

* `${catalog}` - Nome do cat√°logo
* `${schema_bronze}` - Schema bronze (ex: `01_bronze`)
* `${schema_silver}` - Schema silver (ex: `02_silver`)

> Observa√ß√£o: as Tasks 3 e 4 precisam apenas de `${catalog}` e `${schema_silver}`.

**Sa√≠das:**

* Tabela: `${catalog}.${schema_silver}.claims_enriched`
* Cont√©m todos os campos de claims + campos de policies + campos de customers
* Inclui metadados: `processed_at` (timestamp de processamento)
* M√©tricas e amostras entregues em tasks espec√≠ficas

### Configura√ß√£o no Lakeflow Job

**Depend√™ncias entre tasks:**
* Task 2 depende da Task 1 (deve executar ap√≥s a deduplica√ß√£o)
* Task 3 depende da Task 2
* Task 4 depende da Task 2 (ou da Task 3, conforme a necessidade do workflow)

**Vari√°veis a configurar nas tasks:**

| Vari√°vel        | Valor Exemplo    | Descri√ß√£o                    |
|-----------------|-------------------|------------------------------|
| `catalog`       | `smart_claims_dev`| Nome do cat√°logo Unity Catalog|
| `schema_bronze` | `01_bronze`      | Schema de origem (bronze) - usado nas Tasks 1 e 2 |
| `schema_silver` | `02_silver`      | Schema de destino (silver)   |

### Estrutura dos Scripts

* **Task 1 - Deduplica√ß√£o (Python):** arquivo `01_deduplicate_claims.py`, usa PySpark para aplicar `ROW_NUMBER()`, persistir `claims_dedup` e registrar m√©tricas via `taskValues`.
* **Task 2 - Cria√ß√£o da tabela enriched:** arquivo `02_join_tables_create.sql`, materializa `claims_enriched` com o join completo.
* **Task 3 - M√©tricas da tabela enriched:** arquivo `03_join_tables_metrics.sql`, calcula contagens e unicidade para valida√ß√£o.
* **Task 4 - Amostra da tabela enriched:** arquivo `04_join_tables_sample.sql`, retorna uma amostra dos dados resultantes.

### Nota Importante

Este exemplo usa **ingest√£o batch simples** com `CREATE TABLE AS`, diferente dos pipelines DLT (Delta Live Tables) que est√£o em `notebooks/dlt/`. Os scripts batch s√£o ideais para:

* Processamento de arquivos j√° dispon√≠veis em volumes
* Workflows mais simples e diretos
* Casos onde n√£o √© necess√°rio streaming incremental
* Reprocessamento completo quando novos arquivos chegam com o mesmo schema