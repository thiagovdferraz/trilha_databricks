# üß† Aula 06 ‚Äî Spark Declarative Pipelines (SDP)

### *Parte Te√≥rica ‚Äì Sem C√≥digo*

## üìå 1. O que s√£o Spark Declarative Pipelines?

Spark Declarative Pipelines (SDP) s√£o a nova abordagem da Databricks para constru√ß√£o de pipelines de dados de maneira **declarativa**, simplificando a forma como engenheiros criam, operam e escalam pipelines batch e streaming.

Eles s√£o a evolu√ß√£o do antigo **Delta Live Tables**, agora integrados ao ecossistema do **Lakeflow**, unificando ingest√£o, transforma√ß√£o, governan√ßa e orquestra√ß√£o dentro da Data Intelligence Platform.

A ideia central √© simples:

### *Voc√™ declara o que o pipeline precisa fazer e o Databricks cuida do resto.*

---

## üìå 2. Por que Spark Declarative Pipelines existem?

As imagens refor√ßam que pipelines tradicionais sofrem com:

### ‚ùå Desenvolvimento trabalhoso

* Muito c√≥digo boilerplate
* Cria√ß√£o manual de roteiros, triggers, capturas de erros e checkpoints

### ‚ùå Complexidade operacional

* Alto custo para manter
* Dificuldade em orquestrar tarefas
* Fragilidade em caso de falhas

### ‚ùå Workloads isolados

* L√≥gica separada para batch e streaming
* Arquitetura duplicada e dif√≠cil de manter

Essas limita√ß√µes levam a:

* Pipelines lentos
* Alto custo operacional
* Baixa flexibilidade

---

## üìå 3. A proposta das Declarative Pipelines

A vis√£o da Databricks √© clara:

### ‚úî *SDP tornam pipelines confi√°veis mais f√°ceis de construir e operar.*

Isso acontece porque grande parte da ‚Äúinfraestrutura invis√≠vel‚Äù √© assumida automaticamente pelo sistema:

* Controle de vers√£o
* Qualidade dos dados
* Governan√ßa
* Infraestrutura de deploy
* Particionamento
* Checkpointing & retries
* Gerenciamento de depend√™ncias
* Data discovery

Tudo isso √© autom√°tico.

Assim, o foco do engenheiro passa a ser apenas:

### üëâ **A l√≥gica de transforma√ß√£o**

Nada mais.

---

## üìå 4. Benef√≠cios principais das Declarative Pipelines

### **üîπ 1. Simplifica√ß√£o radical na cria√ß√£o de pipelines**

* Voc√™ declara entradas e sa√≠das
* O Databricks cria a infraestrutura necess√°ria
* Funciona tanto para batch quanto para streaming

### **üîπ 2. Otimiza√ß√µes autom√°ticas no n√≠vel do cluster**

* Escalonamento autom√°tico
* Recupera√ß√£o de falhas
* Menos retrabalho manual
* Redu√ß√£o de custos

### **üîπ 3. Batch e streaming unificados**

A mesma defini√ß√£o funciona para:

* Workloads em lote
* Workloads em streaming
* Materialized views
* Incremental processing

Isso reduz drasticamente pipelines duplicados.

### **üîπ 4. Opera√ß√£o mais confi√°vel**

* Dados com n√≠veis de qualidade Bronze ‚Üí Silver ‚Üí Gold
* Pipelines autogerenci√°veis
* Lineage e governan√ßa via Unity Catalog

### **üîπ 5. Desenvolvimento acelerado**

* Menos boilerplate
* Menos depend√™ncia em notebooks gigantes
* Melhor manuten√ß√£o

---

## üìå 5. Como criar uma Declarative Pipeline (vis√£o te√≥rica)

A cria√ß√£o pode ser feita de duas maneiras (mostradas nas imagens):

### **Op√ß√£o 1 ‚Äî Pelo Workspace**

* Clique no menu de contexto
* Selecione *Create ‚Üí ETL Pipeline*

### **Op√ß√£o 2 ‚Äî Pelo menu Jobs & Pipelines**

* V√° em *Jobs & Pipelines*
* Clique em *Create*
* Escolha *ETL Pipeline*

O Databricks ent√£o abre a interface dedicada, onde o usu√°rio define:

* Nome da pipeline
* Local das tabelas
* Configura√ß√µes de cluster
* Estrat√©gias de processamento

---

## üìå 6. Pipeline Declarativo na arquitetura Medallion

As imagens refor√ßam como o SDP opera naturalmente no modelo Bronze ‚Üí Silver ‚Üí Gold.

### **Bronze**

* Tabelas de ingest√£o
* Raw data
* Entrada em streaming ou batch

### **Silver**

* Dados limpos
* Normalizados
* Com governan√ßa e qualidade aplicada

### **Gold**

* Agrega√ß√µes de neg√≥cio
* M√©tricas
* Materialized views
* Bases para BI, AI e Analytics

O SDP:

* Controla o fluxo incremental automaticamente
* Identifica novos arquivos (file 1, file 2‚Ä¶)
* Processa apenas o que mudou
* Mant√©m hist√≥rico e atomicidade

---

## üìå 7. Incremental Processing

Um ponto muito importante:

### **Spark Declarative Pipelines processam dados incrementalmente por padr√£o.**

Isso significa:

* A cada execu√ß√£o, apenas novos dados s√£o processados
* Tabelas Bronze e Silver funcionam como tabelas de streaming
* Gold funciona como vis√£o materializada (materialized view)

E tudo isso √© otimizado automaticamente no Serverless.

---

## üìå 8. Conex√£o com fontes de dados (Lakeflow Connect)

Outra parte da teoria crucial:

Declarative Pipelines podem receber dados de:

* **Cloud Storage** (S3, ADLS, GCS)
* **Message Queues** (Kafka, Pub/Sub, Kinesis)
* **Databases** (SQL Server, PostgreSQL, etc.)
* **SaaS** (Salesforce, Workday, etc.)

O Lakeflow Connect fornece conectores otimizados com:

* Auto-infer√™ncia
* Auto-schema
* Auto-refresh
* Retry e controle de offset

---

## üìå 9. Onde o SDP se encaixa na Data Intelligence Platform

Resumindo tudo:

* **Governan√ßa:** Unity Catalog
* **Armazenamento:** Delta Lake / Parquet / Iceberg
* **Execu√ß√£o:** Spark + Structured Streaming
* **Ingest√£o:** Lakeflow Connect
* **Transforma√ß√£o:** Spark Declarative Pipelines
* **Orquestra√ß√£o:** Lakeflow Jobs

Tudo dentro de um ecossistema unificado que reduz custo, complexidade e esfor√ßo de manuten√ß√£o.

---

## üìå 10. O papel do engenheiro de dados com SDP

Com a introdu√ß√£o dos SDP, o papel muda:

### Antigo estilo

* Engenheiro escrevia tudo √† m√£o
* Precisava cuidar de falhas
* Ajustava manualmente cluster e otimiza√ß√µes
* Tratava batch e streaming como mundos separados

### Novo estilo

* Engenheiro **declara o pipeline**
* O Databricks cuida da infraestrutura
* Qualidade, lineage e reprocessamento s√£o autom√°ticos
* O foco vira **modelagem, governan√ßa e valor de neg√≥cio**

---

# üéØ **Conclus√£o da Aula Te√≥rica**

Spark Declarative Pipelines representam a nova forma de construir ETL dentro da arquitetura Lakehouse. Eles simplificam todo o processo:

* Menos manuten√ß√£o
* Menos custo
* Menos retrabalho
* Mais performance
* Mais confiabilidade

E principalmente:

### üëâ **Voc√™ foca no valor ‚Äî o Databricks foca na engenharia.**

---

## üõ†Ô∏è Parte Pr√°tica ‚Äî Pipeline DLT

## üìã Objetivo

Recriar o pipeline da **Aula 05** usando **Spark Declarative Pipelines (DLT)**, demonstrando como a mesma l√≥gica pode ser implementada de forma mais simples e declarativa.

## üìä Datasets Utilizados

Esta aula utiliza os **mesmos datasets da Aula 05**:

* `customers.csv` - Dados de clientes
* `policies.csv` - Dados de ap√≥lices
* `claims.csv` e `claims_02.csv` - Dados de sinistros

**Localiza√ß√£o dos dados:** `/Volumes/smart_claims_dev/00_landing/sql_server/`

Os dados est√£o organizados em pastas separadas:
* `customers/` - Pasta com arquivos CSV de customers
* `policies/` - Pasta com arquivos CSV de policies
* `claims/` - Pasta com arquivos CSV de claims (claims.csv, claims_02.csv, etc.)

> üí° **Nota:** Os datasets devem estar dispon√≠veis no volume configurado na Aula 05. Se necess√°rio, execute os notebooks de setup da Aula 05 antes de iniciar esta aula.

## üìÅ Estrutura de Schemas

O pipeline utiliza a seguinte estrutura de schemas no Unity Catalog:

* **`00_landing`** - Volume com arquivos CSV brutos (fonte dos dados)
* **`01_bronze`** - Tabelas bronze com dados brutos ingeridos
* **`02_silver`** - Tabelas silver com dados limpos e enriquecidos
* **`03_gold`** - Views materializadas gold com m√©tricas e agrega√ß√µes

Todos os c√≥digos referenciam explicitamente os schemas completos no formato: `smart_claims_dev.{schema}.{tabela}`

## üèóÔ∏è Estrutura do Pipeline DLT

O pipeline est√° organizado na pasta `pipeline_lakeflow/transformations/` seguindo a arquitetura Medallion:

### üì¶ Camada Bronze (Ingest√£o)

Todas as tabelas bronze usam **CREATE OR REFRESH TABLE** com ingest√£o batch:

1. **`01_bronze_customers.sql`** - Ingest√£o de dados de customers usando `read_files()` com pasta `customers/`
2. **`02_bronze_policies.sql`** - Ingest√£o de dados de policies usando `read_files()` com pasta `policies/`
3. **`03_bronze_claims.sql`** - Ingest√£o de dados de claims usando `read_files()` com pasta `claims/` (processa todos os arquivos da pasta)

**Caracter√≠sticas:**
* Cada pasta √© ingerida separadamente preservando a organiza√ß√£o dos arquivos
* Usa apenas o caminho da pasta (n√£o precisa especificar o nome do arquivo)
* O `read_files()` processa automaticamente todos os arquivos CSV dentro de cada pasta
* Processamento batch com `CREATE OR REFRESH` para reprocessamento autom√°tico

### ‚ú® Camada Silver (Transforma√ß√£o e Limpeza)

A camada silver usa **CREATE OR REFRESH TABLE**:

1. **`04_silver_claims_enriched.sql`** - Enriquecimento de claims atrav√©s de joins diretos com policies e customers

**Caracter√≠sticas:**
* Processamento batch com l√≥gica de transforma√ß√£o
* Joins diretos entre tabelas bronze usando endere√ßos completos (`smart_claims_dev.01_bronze.*`)
* Refer√™ncias expl√≠citas aos schemas completos para maior clareza

### üèÜ Camada Gold (Agrega√ß√µes e M√©tricas)

A camada gold usa **MATERIALIZED VIEW** para agrega√ß√µes otimizadas:

1. **`05_gold_claims_metrics.sql`** - View materializada com m√©tricas agregadas usando endere√ßo completo da tabela silver

**Caracter√≠sticas:**
* Agrega√ß√µes otimizadas automaticamente
* Atualiza√ß√£o incremental quando poss√≠vel
* Consultas r√°pidas para BI e Analytics

## üîÑ Compara√ß√£o: Aula 05 vs Aula 06

| Aspecto | Aula 05 (Lakeflow Jobs) | Aula 06 (DLT) |
|---------|------------------------|---------------|
| **Sintaxe** | `CREATE TABLE ... AS SELECT` | `CREATE OR REFRESH TABLE` (Bronze/Silver) e `MATERIALIZED VIEW` (Gold) |
| **Orquestra√ß√£o** | Lakeflow Jobs com m√∫ltiplas tasks | Pipeline √∫nico declarativo |
| **Transforma√ß√£o** | Joins manuais entre tabelas | Joins declarativos entre tabelas bronze |
| **Depend√™ncias** | Gerenciadas manualmente no Job | Gerenciadas automaticamente pelo DLT |
| **Reprocessamento** | DROP TABLE manual antes de recriar | `CREATE OR REFRESH` autom√°tico |
| **Ingest√£o** | Caminhos espec√≠ficos por arquivo CSV | Caminhos espec√≠ficos por arquivo CSV (preserva nomes) |
| **Manuten√ß√£o** | M√∫ltiplos arquivos e configura√ß√µes | Pipeline √∫nico e declarativo |
| **Bronze** | Tabelas batch com DROP/CREATE | Tabelas batch com CREATE OR REFRESH |
| **Silver** | Tabelas batch | Tabelas batch com CREATE OR REFRESH |
| **Gold** | Views est√°ticas | MATERIALIZED VIEW com otimiza√ß√µes autom√°ticas |

## üöÄ Como Executar o Pipeline

### Pr√©-requisitos

1. ‚úÖ Ter executado a **Aula 05** para configurar:
   * Cat√°logo e schemas (bronze, silver, gold)
   * Volume com os datasets
   * Estrutura b√°sica do Unity Catalog

2. ‚úÖ Acessar o workspace Databricks com permiss√µes para:
   * Criar pipelines DLT
   * Ler volumes
   * Criar tabelas nos schemas configurados

### Passos para Execu√ß√£o

1. **Acesse a pasta `pipeline_lakeflow/transformations/`**

2. **Execute os arquivos na ordem:**
   * Use `Run file` para testar cada transforma√ß√£o individualmente
   * Use `Run pipeline` para executar todo o pipeline de uma vez

3. **Cat√°logo e Schemas:**
   * **Cat√°logo:** `smart_claims_dev`
   * **Schemas:** `01_bronze`, `02_silver`, `03_gold`
   * Todos os c√≥digos referenciam explicitamente: `smart_claims_dev.{schema}.{tabela}`

4. **Monitore a execu√ß√£o:**
   * Acompanhe o progresso na interface do DLT
   * Verifique as tabelas criadas no Unity Catalog
   * Consulte a view `smart_claims_dev.03_gold.claims_metrics` para validar os resultados

## üìà Resultados Esperados

Ap√≥s a execu√ß√£o do pipeline, voc√™ ter√°:

* ‚úÖ **Schema `01_bronze`:** 3 tabelas (customers, policies, claims)
* ‚úÖ **Schema `02_silver`:** 1 tabela (claims_enriched)
* ‚úÖ **Schema `03_gold`:** 1 view materializada (claims_metrics)

Todas as tabelas estar√£o dispon√≠veis no Unity Catalog e podem ser consultadas normalmente via SQL usando a refer√™ncia completa: `smart_claims_dev.{schema}.{tabela}`

## üí° Vantagens do DLT Demonstradas

1. **Simplicidade:** Menos c√≥digo, mesma funcionalidade
2. **Manutenibilidade:** Pipeline √∫nico e declarativo
3. **Confiabilidade:** Gerenciamento autom√°tico de depend√™ncias
4. **Performance:** Reprocessamento eficiente com `CREATE OR REFRESH`
5. **Governan√ßa:** Integra√ß√£o nativa com Unity Catalog
6. **Preserva√ß√£o de Arquivos:** Cada CSV √© ingerido separadamente preservando os nomes dos arquivos
7. **Otimiza√ß√µes Autom√°ticas:** MATERIALIZED VIEW na camada Gold com agrega√ß√µes otimizadas
8. **Reprocessamento Autom√°tico:** `CREATE OR REFRESH` permite reprocessar dados sem DROP manual

## üîç Explorando o Pipeline

Use a pasta `explorations/` para criar notebooks ad-hoc e explorar os dados processados pelo pipeline. Exemplo:

```sql
-- Explorar dados bronze
SELECT * FROM smart_claims_dev.01_bronze.customers LIMIT 10;

-- Verificar total de registros
SELECT COUNT(*) FROM smart_claims_dev.01_bronze.claims;
SELECT COUNT(*) FROM smart_claims_dev.02_silver.claims_enriched;

-- Consultar m√©tricas
SELECT * FROM smart_claims_dev.03_gold.claims_metrics;
```

---

# üìö **Recursos Adicionais**

* [Documenta√ß√£o DLT](https://docs.databricks.com/dlt/)
* [Sintaxe SQL Reference DLT](https://docs.databricks.com/dlt/sql-ref.html)
* [Best Practices DLT](https://docs.databricks.com/dlt/best-practices.html)
